---
title: "Method Validation in R"
author: 
 - "Daniel Herman"
 - "Niklas Krumm"
date: "06/07/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NA)
library(tidyverse)
library(readxl)
# library(janitor)
```

# Overview

In this section we will walk through examples of how to use R to analyze standard method validation experiments. We will use real data acquired as part of a method transition for immunoassays (unconjugated estriol, alpha fetoprotein, and inhibin-A) used for prenatal assessment of Trisomy 21, Trisomy 18, and open neural tube defect risk assessment.

# Load data

Relatively "clean" data has already been organized in separate tabs in an excel document. Let's first load one of these data sets. To do this we will use the `read_excel` function. Let's first take a look at this function. Click the green arrow to execute this code chunk:

```{r explore_function}
?read_excel
```

We see that there are three different functions grouped together, a `read_excel` function that is a wrapper for two more specific functions `read_xls` and `read_xlsx`. Since our Excel document is in .xlsx format, we can either use the general `read_excel` or `read_xlsx` function. We need to specify the path to our excel file and the worksheet we would like to load in. Here is an example:

```{r example_data_load}
tmp <- read_excel(path="data/Method_Validation.data.xlsx", 
                sheet="MS uE3")
tmp  # Take a peak
```


> **Exercise:**
> Load in the alpha fetoprotein (AFP) method comparison data into a tbl named `data`

```{r load_data_1, echo=FALSE}
data <- read_excel(path="data/Method_Validation.data.xlsx", 
                sheet="MS AFP")
data
```

There are lots of different options for loading in files in R. For tab-delimited text, try `read_tsv`. For comma-delimited text (.csv), try `read_csv`. See more detailed discussion in [MSACL intermediate course](https://github.com/pcmathias/MSACL-intermediate-R-course/tree/master/lesson3).

# Viewing data tables

## QC data import
Once you have loaded some data into an R variable (in this case, `data`), you should always briefly visually and manually inspect it. Things to check for include:

 - Did the column headers import properly?  [Need to be able to refer to each column]
 - Did columns get split appropriately [Do we have the right number of columns]
 - Did any lines get skipped? Did the entire file get loaded?

## Discuss the tbl data structure
Let's next take a look at the data we have now saved as `data`. We can do this in a few different ways.

```{r}
data
```


Note that it looks very similar to its original form in Excel. There are 3 columns and 161 rows. Each column has a specfic data format (`chr`, `dbl`, etc.). The `read_exel` function assigned these automatically. For our purposes, it is important that our result columns are numeric.

In R-Studio, variables can also be _inspected_ using the "Environment" tab (usually in the top right pane). 

> **Exercise:** Click on `data`.

## Summarizing a data table
The `summary` command can provide a helpful overview of the data set as well:

```{r}
summary(data)
```

 
## Working with tables in R:

A quick way to look at only the top `n` rows is using the `head` function:
```{r}
# top 3 rows
head(data, n=3)
```

> _Comments: Any line that starts with a "#" is considered a "comment" in R, and **is not part of the code**. You can use comments to help explain to others (or future-you) what a particular section of code is doing, or to quickly "comment out" a portion of the code that you do not want to run. Comments can also go at the end of a line._

If you wish to show only certain columns, you can specify this with `select`:

```{r}
select(data, method_a, method_b)   # Select the method_a and methob_b columns from the tbl data
```

Advanced topic: What if we want to select columns AND rows? Here are two approaches to this:

*First approach using intermediate/temporary variables:*
```{r}
temp_table <- select(data, specimen, method_a)
final_table <- head(temp_table, n=3)
final_table
```

*Second approach using pipes:*
```{r}
data %>%
  select(specimen, method_a) %>%
  head(n=3)
```

Note that `data` is not listed explicitly in the `select` and `head` commands. Rather, the result of each command is passed along by the pipe (`%>%`) to following command. Pipes make for cleaner, simpler code and work for `tidyverse` and many other R functions.

> **Exercise:**
> Use `select` and `tail` to choose The last 20 rows of `method_b` results

```{r, echo=FALSE}
temp_table <- data %>%
                tail(n=20) %>%
                select(method_b)
temp_table
```

Finally, we can use the `$` shorthand to isolate _individual_ columns. Note that this is no longer a table, but just an array of values.
```{r}
data$method_a
```

# Examine data distribution

Let's focus on the maternal serum AFP values from Method A first. 

```{r basic_stats}
mean(x = data$method_a)  # Mean
median(x = data$method_a) # Median
sd(x = data$method_a) # Standard deviation
```

What about the coefficient of variation `CV% = mean(x) / sd(x) * 100`?
```{r}
mu <- mean(x = data$method_a)  # Mean
s <- sd(x = data$method_a) # Standard deviation
s/mu * 100
```

*In calling functions (i.e. `mean`), the values can be passed in order the functions arguments, or they can be explicitly assigned (`arg` = `value`). In general, it is good practice to specify the argument receiving each value, because this is safer and easier to follow. However, because this makes statement much longer, it is common to not explicitly list arguments for common functions with few, easily interpretable arguments. As an example, since `mean` and `sd` only take single values, we can simply write and read:*

```{r}
sd(data$method_a) / mean(data$method_a) * 100  # Coefficient of variation
```


## Is it normally distributed?
There are many approaches to determining "normality" of a distribution. Calculation of mean, median, mode and SDs can help. Visual inspection of the data will also quickly reveal if the data is normal, skewed, multimodal, etc. 

> A famous example of how summary statistics can mask true differences is demonstrated by Anscombe's quartet. FJ Anscombe, an English statistician, famously said in 1973 that "Computers should make both calculations and graphs. Both sorts of output should be studied; each will contribute to understanding." An entertaining (involving T-rex) and informative visualization of this problem can be found here: https://www.autodeskresearch.com/publications/samestats

We will use the `ggplot` library to visualize data.

> The idea of ggplot is to use graphical "building blocks" and combine them to create just about any kind of graphical display you want. Here is an [in-depth tutorial of ggplot](https://tutorials.iq.harvard.edu/R/Rgraphics/Rgraphics.html).

``` {r}
g <-  ggplot(data, aes(x=method_a))
g <- g + geom_histogram(bins=50)
g
```

Does this look normal? 

> **Exercise**: Transform the x-axis to log-scale using `scale_x_log10` and visually inspect for normality

```{r echo=FALSE}
g <-  ggplot(data, aes(x=method_a))
g <- g + geom_histogram(bins=50)
g <- g + scale_x_log10()   # Change x-axis scaling
g
```



How can we quantify this?

The **skew** and **kurtosis** are so-called **moments** of a distribution and can be used as measure of normality (or non-normality).

```{r}
library(moments)            # load the moments R library, which provides the functions
skewness(data$method_a)
kurtosis(data$method_a)
```

> **Exercise:** Let's assess the `skewness` of the log-transformed method_a data. Use the `log10` function to transform results.

```{r, echo=FALSE}
method_a_logged <- log10(data$method_a)
skewness(method_a_logged)
```


Another option is to use the **Shapiro-Wilk test of normality**:

```{r}
shapiro.test(data$method_a)
```

## How to establish reference range?
Reference ranges can be established in several different ways. Two of the most common approaches are termed **parametric** and **non-parametric**. 

| |Parametric|Non-parametric|
|:-------------|:-------------------------:|:-------------------------:|
|Good for|Normal distributions|Skewed, log-normal, or other non-normal distributions|
|Assumptions|Assumes normality|No assumptions about underlying distribution|
|Center of distribution|Mean|Median|
|Advantages|More power|Less affected by outliers, Simple to calculate|
|Disadvantages|Affected by outliers + skew|Less power requires more samples|
|CLSI Recommended Approach|No|Yes|

### Parametric Reference Ranges

![](assets/rrange.png){width=350px}

\[
range = \mu Â±1.96* \sigma
\]
Where $\mu$ is the mean (average) of the distribution and $\sigma$ is the standard deviation.

We can calculate these for our results in `method_a` using R:

```{r}
mu <- mean(data$method_a)
sigma <- sd(data$method_a)

lower_bound <- mu - 1.96 * sigma
upper_bound <- mu + 1.96 * sigma

sprintf("The reference range assuming normal distribution is (%f, %f)", lower_bound, upper_bound)
```

> **Exercise:** Calculate the parametric reference range assuming that the results are log-normal (rather than normally distributed)

```{r echo=FALSE}
mu <- mean(log(data$method_a))
sigma <- sd(log(data$method_a))

lower_bound_transformed <- mu - 1.96 * sigma
upper_bound_transformed <- mu + 1.96 * sigma

lower_bound_untransformed <- exp(lower_bound_transformed)
upper_bound_untransformed <- exp(upper_bound_transformed)

sprintf("The reference range assuming log-normal distribution is (%f, %f)", lower_bound_untransformed, upper_bound_untransformed)

```

Now, let's inspect these proposed bounds graphically. To add a vertical line use the function `geom_vline(xintercept=XXX)`

```{r}
g <-  ggplot(data, aes(x=method_a))
g <- g + geom_histogram(bins=50)
g <- g + geom_vline(xintercept=lower_bound_untransformed, color="blue")
g
```

> **Exercise:** Add the additional 3 bounds we have just calculated to this plot. Chance to a dashed line by specifying `linetype=3` in the `geom_vline` function.

```{r, echo=FALSE}
g <-  ggplot(data, aes(x=method_a))
g <- g + geom_histogram(bins=50)
g <- g + geom_vline(xintercept=lower_bound_untransformed, linetype=3, color="blue")
g <- g + geom_vline(xintercept=upper_bound_untransformed, linetype=3, color="blue")
g <- g + geom_vline(xintercept=lower_bound, linetype=3, color="red")
g <- g + geom_vline(xintercept=upper_bound, linetype=3, color="red")
g
```


### Non-parametric Reference Ranges
Non-parametric reference ranges are simply the middle 95% of the distribution. Using R, this is done with the straightforward command `quantile`:

```{r}
non_parametric_bounds <- quantile(x=data$method_a, probs = c(0.025, 0.975))
print(non_parametric_bounds)
```

Can we verify that the `non_parametric_bounds` includes 95% of the data? Sure thing! We will do this with some new operations and a feature in R called masking. First we need to be able to refer to each bound separately. Don't want to get lost in the weeds here, but not several different ways of extracting elements of the vector `non_parametric_bounds`

```{r}
non_parametric_bounds
non_parametric_bounds[1]
non_parametric_bounds[[1]]
non_parametric_bounds[["2.5%"]]
```

Let's first ask which elements of `method_a` are greater than our lower-bound?
```{r}
mask <- data$method_a > non_parametric_bounds[[1]]
mask
```

The result is a boolean (TRUE/FALSE) array. Then lets combine our upper and lower bounds
```{r}
mask <- (data$method_a > non_parametric_bounds[[1]]) & 
                (data$method_a < non_parametric_bounds[[2]])
mask
```

This intermediate data structure `mask` is convenient, because we can easily ask how many elements it contains and how many are `TRUE`:
```{r}
length(mask)
sum(mask == TRUE)
proportion_in_range = sum(mask == TRUE) / length(mask)
print(proportion_in_range)

#Alternative
sum(mask) / length(mask)
```


> **Exercise:** Plot these proposed reference intervals on histogram
```{r echo=FALSE}
g <-  ggplot(data, aes(x=method_a))
g <- g + geom_histogram(bins=50)
g <- g + geom_vline(xintercept = non_parametric_bounds["2.5%"], color="red", linetype=3)
g <- g + geom_vline(xintercept = non_parametric_bounds["97.5%"], color="red", linetype=3)
g
```


## How to verify a proposed reference range?

The lab had previously used a reference range of 20-100 for this particular test. 

```{r}
g <-  ggplot(data, aes(x=method_a))
g <- g + geom_histogram(bins=50)
g <- g + geom_vline(xintercept = 20, color="red", linetype=3)
g <- g + geom_vline(xintercept = 100, color="red", linetype=3)
g
```

Is this range appropriate for our observed results?
```{r}

n_samples <- length(data$method_a)

old_range <- c(20, 100) #proposed range
n_inrange <- with(data,
                  sum(method_a >= old_range[1] & method_a <= old_range[2]))

chisq.test(c(n_inrange, n_samples - n_inrange), p=c(0.95, 0.05))
binom.test(x=n_inrange, n=n_samples, p = 0.95, alternative = "two.sided")

```

> **Exercise:** Let's evaluate our empirically considered parametric reference ranges

```{r, echo=FALSE}
n_samples <- nrow(data)
ref_range <- c(lower_bound, upper_bound)
sprintf("The reference range assuming normal distribution is (%f, %f)", ref_range[1], ref_range[2])
n_inrange <- with(data,
                  sum(method_a >= ref_range[1] & method_a <= ref_range[2]))
binom.test(x=n_inrange, n=n_samples, p = 0.95, alternative = "two.sided")


n_samples <- nrow(data)
ref_range <- c(lower_bound_untransformed, upper_bound_untransformed)
sprintf("The reference range assuming log-normal distribution is (%f, %f)", ref_range[1], ref_range[2])
n_inrange <- with(data,
                  sum(method_a >= ref_range[1] & method_a <= ref_range[2]))
binom.test(x=n_inrange, n=n_samples, p = 0.95, alternative = "two.sided")
```


# Using a Q-Q plot to examine a distribution
```{r}
ggplot(data) + geom_qq(aes(sample=method_a))
```


Are there outliers? How can you remove them?

```{r}
upper_outlier_bound <- mean(data$method_a) + 3 * IQR(data$method_a)

removed_outliers_dat <- filter(data, 
                               method_a < upper_outlier_bound)
ggplot(removed_outliers_dat) + geom_qq(aes(sample=method_a)) 

```
