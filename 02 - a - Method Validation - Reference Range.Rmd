---
title: "Method Validation in R"
author: "Daniel Herman"
date: "06/07/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NA)
library(tidyverse)
library(readxl)
library(janitor)
```

# Overview

In this section we will walk through examples of how to use R to analyze standard method validation experiments. We will use real data acquired as part of a method transition for immunoassays (unconjugated estriol, alpha fetoprotein, and inhibin-A) used for prenatal assessment of Trisomy 21, Trisomy 18, and open neural tube defect risk assessment.

# Load data

Relatively "clean" data has already been organized in separate tabs in an excel document. Let's first load one of these data sets. To do this we will use the `read_excel` function. Let's first take a look at this function. Click the green arrow to execute this code chunk:

```{r explore_function}
?read_excel
```

We see that there are three different functions grouped together, a `read_excel` function that is a wrapper for two more specific functions `read_xls` and `read_xlsx`. Since our Excel document is in .xlsx format, we can either use the general `read_excel` or `read_xlsx` function.

```{r load_data_1}
data <- read_excel(path="data/Method_Validation.data.xlsx", 
                sheet="MS AFP")
```

There are lots of different options for loading in files in R. For tab-delimited text, try `read_tsv`. For comma-delimited text (.csv), try `read_csv`. See more detailed discussion in [MSACL intermediate course](https://github.com/pcmathias/MSACL-intermediate-R-course/tree/master/lesson3).

## (Advanced) Discuss the tbl data structure
Mention other structures

# Viewing data tables
Once you have loaded some data into an R variable (in this case, `dat`), you should always briefly visually and manually inspect it. Things to check for include:

 - Did the column headers import properly?
 - Did columns get split appropriately, and does every line have the right number of columns?
 - Did any lines get skipped? Did the entire file get loaded?
 
Let's next take a look at the data. We can do this in a few different ways.

```{r}
data
```

In R-Studio, variables can also be _inspected_ using the "Environment" tab (usually in the top right pane). 

## Summarizing a data table
The `summary` command can provide a helpful overview of the data set as well:

```{r}
summary(data)
```

## Working with tables in R:

A quick way to look at only the top `n` rows is using the `head` function:
```{r}
# top 3 rows
head(data, n=3)
```

> _Comments: Any line that starts with a "#" is considered a "comment" in R, and **is not part of the code**. You can use comments to help explain to others (or future-you) what a particular section of code is doing, or to quickly "comment out" a portion of the code that you do not want to run. Comments can also go at the end of a line._

If you wish to show only certain columns, you can specify this with `select`:

```{r}
select(data, method_a, method_b)
```

Advanced topic: What if we want to select columns AND rows? There are two approaches to this:

*First approach using intermediate/temporary variables:*
```{r}
temp_table <- select(data, specimen, method_a)
final_table <- head(temp_table, n=3)
final_table
```

*Second approach using pipes:*
```{r}
data %>%
  select(specimen, method_a) %>%
  head(n=3)
```

Finally, we can use the `$` shorthand to isolate _individual_ columns. Note that this is no longer a table, but just an array of values.
```{r}
data$method_a
```

# Examine data distribution

Let's focus on the maternal serum AFP values from Method A first.

```{r basic_stats}
mean(data$method_a)  # Mean
median(data$method_a) # Median
sd(data$method_a) # Standard deviation
sd(data$method_a) / mean(data$method_a) * 100  # Coefficient of variation
```

## Is it normally distributed?
There are many approaches to determining "normality" of a distribution. Calculation of mean, median, mode and SDs can help. Visual inspection of the data will also quickly reveal if the data is normal, skewed, multimodal, etc. 

> A famous example of how summary statistics can mask true differences is demonstrated by Anscombe's quartet. FJ Anscombe, an English statistician, famously said in 1973 that "Computers should make both calculations and graphs. Both sorts of output should be studied; each will contribute to understanding." An entertaining (involving T-rex) and informative visualization of this problem can be found here: https://www.autodeskresearch.com/publications/samestats

We will use the `ggplot` library to visualize data.
> The idea of ggplot is to use graphical "building blocks" and combine them to create just about any kind of graphical display you want. Here is an (in-depth tutorial of ggplot)[https://tutorials.iq.harvard.edu/R/Rgraphics/Rgraphics.html]

``` {r}
g <-  ggplot(data, aes(x=method_a))
g <- g + geom_histogram(bins=50)
g
```

Does this look normal? How can we quantify this?

The **skew** and **kurtosis** are so-called **moments** of a distribution and can be used as measure of normality (or non-normality).

```{r}
library(moments)            # load the moments R library, which provides the functions
skewness(data$method_a)
kurtosis(data$method_a)
```

Another option is to use the **Shapiro-Wilk test of normality**:

```{r}
shapiro.test(data$method_a)
```

-- May want to add Gender, Gestational week here...

## How to establish reference range?
Reference ranges can be established in several different ways. Two of the most common approaches are termed **parametric** and **non-parametric**. 

| |Parametric|Non-parametric|
|:-------------|:-------------------------:|:-------------------------:|
|Good for|Normal distributions|Skewed, log-normal, or other non-normal distributions|
|Assumptions|Assumes normality|No assumptions about underlying distribution|
|Center of distribution|Mean|Median|
|Advantages|More power|Less affected by outliers, Simple to calculate|
|Disadvantages|Affected by outliers + skew|Less power requires more samples|
|CLSI Recommended Approach|No|Yes|

### Parametric Reference Ranges

![](assets/rrange.png){width=350px}

\[
range = \mu Â±1.96* \sigma
\]
Where $\mu$ is the mean (average) of the distribution and $\sigma$ is the standard deviation.

We can calculate these for our results in `method_a` using R:

```{r}
mu <- mean(data$method_a)
sigma <- sd(data$method_a)

lower_bound <- mu - 1.96 * sigma
upper_bound <- mu + 1.96 * sigma

sprintf("The reference range is (%f, %f)", lower_bound, upper_bound)
```

### Non-parametric Reference Ranges
Non-parametric reference ranges are simply the middle 95% of the distribution. Using R, this is done with the straightforward command `quantile`:

```{r}
non_parametric_bounds <- quantile(x=data$method_a, probs = c(0.025, 0.975))
print(non_parametric_bounds)
```

Can we verify that the `non_parametric_bounds` includes 95% of the data? Sure thing! We will do this with some new operations and a feature in R called masking:
```{r}
mask <- (data$method_a > non_parametric_bounds["2.5%"]) & (data$method_a < non_parametric_bounds["97.5%"])
sum(mask)
length(mask)
proportion_in_range = sum(mask) / length(mask)
print(proportion_in_range)
```


### Advanced plotting exercise: Plot reference intervals on histogram
Say we want to graphically show the bounds of the reference intervals on the histogram we made above.
A simple way to do this is to use the `geom_vline` function of ggplot. 

*Exercise*

1. Look up the function call for `geom_vline`. What kind of arguments does it take? 
2. Start with with the code for the histogram above, add additional lines of code to display the bounds of the parametric reference interval. (_Hint: ggplot allows you to "add" new elements to a plot_)
```{r}
g <-  ggplot(data, aes(x=method_a))
g <- g + geom_histogram(bins=50)
g <- g + geom_vline(xintercept = non_parametric_bounds["2.5%"], color="red")
g <- g + geom_vline(xintercept = non_parametric_bounds["97.5%"], color="red")
g
```


## How to verify a proposed reference range?

The lab had previously used a reference range of 20-100 for this particular test. Is this range still valid?

```{r}

n_samples <- length(data$method_a)

old_range <- c(20, 100)
n_inrange <- sum(data$method_a >= old_range[1] & data$method_a <= old_range[2])

chisq.test(c(n_inrange, n_samples - n_inrange), p=c(0.95, 0.05))
binom.test(x=n_inrange, n=n_samples, p = 0.95, alternative = "two.sided")

```

# Using a Q-Q plot to examine a distribution
```{r}
ggplot(data) + geom_qq(aes(sample=method_a))
```


Are there outliers? How can you remove them?

```{r}
upper_outlier_bound <- mean(data$method_a) + 3*IQR(data$method_a)

removed_outliers_data <- filter(data, method_a < upper_outlier_bound)
ggplot(removed_outliers_data) + geom_qq(aes(sample=method_a)) 

```
