---
title: "Method Validation in R -- Precision, Linearity, and calibration verification"
authors: 
 - "Daniel Herman"
 - "Niklas Krumm"
date: "06/07/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NA)
library(tidyverse)
library(readxl)
```

# Method Validation: Precision, Linearity, and Calibration Verification

## Overview 
In this section, we will evaluate analytical method precision, calibration verification, and linearity.

## Precision

### Load data

**Exercise 1:** Load the `Precision` tab of `Method_Validation.data.xlsx` into the object `precision`.
```{r, eval=FALSE}
precision <- 
  
```

```{r, echo=FALSE}
precision <- read_xlsx(path = "data/Method_Validation.data.xlsx", sheet = "Precision")
str(precision)
head(precision)
```
**End exercise**

### Describe data and explore its distribution

Let's figure out what we have got. Looks like there are 7 variables (`r names(precision)`) with a maximum of `r nrow(precision)` observations. However, there are many `NA`'s, which indicates missing values. In this set we have missing data just because it was not loaded into our example dataset, so we can focus on describing the data that is present. There are lots of ways to describe missingess. 

Start by asking what is missing with function `is.na()`.

```{r}
is.na(precision$P2)
```

How many missing values are there? We can count `TRUE`'s like in the previous section using `sum` and `length`:

```{r}
tmp <- is.na(precision)    # Apply `is.na` to each element of `precision`
sum(tmp) / length(tmp) * 100  # Calculate percent missing
sprintf("There are %d (%.0f%%) missing values.", 
        sum(tmp), 
        sum(tmp) / length(tmp) * 100)
```

We can also apply a tidy-er approach: 
```{r}
precision %>%
  map_df(is.na) %>%   # Apply `is.na` to each element
  summarize_all(sum)   # Apply `sum` to each variable
```

**Exercise 2**: 
How many different analytes (`Analyte`) are there? 
- Use the `unique` function to distill an object to a set of unique observations.

```{r, eval=FALSE}


sprintf("There are %d different analytes", __________)
```

```{r, echo=FALSE}
precision$Analyte
unique(precision$Analyte)
sprintf("There are %d different analytes", length(unique(precision$Analyte)))
```
**End exercise**

### Assess precision

Let's focus on the first control `P1`'s inter-day measurements for `AFP`.

To match how we did this in 02a, we can extract this data from the tibble into a vector named `tmp`. Note that following `%>%` the `.` refers to the current version of `tmp`, after enacting the previous lines of code

```{r}
tmp <- precision %>%
  filter(Analyte == "AFP") %>%  # Include only observations of AFP only
  select(P1)  %>%   # Select specific column
  .[["P1"]]      # Extract variable into a vector
tmp
```

Then calculate the `mean`, `sd`, and `CV` as earlier. Note that to format each of the numbers we can use `sprintf` and specify the total number of digits and the number of digits following the decimal (e.g. `%2.1f` refers to a number with 2 totals digits and 1 digit after decimal)

```{r}
mean(tmp)
sd(tmp)
sd(tmp) / mean(tmp) * 100
sprintf("Mean = %2.1f, SD = %2.2f, CV = %3.1f%%",  
        mean(tmp), 
        sd(tmp), 
        sd(tmp) / mean(tmp) * 100)
```

To scale such calculations to multiple variables, it is much easier to do tidy way using `summarize` to calculate the `mean` across all observations of a specific variable `P1`:

```{r}
precision %>%
  filter(Analyte == "AFP") %>%
  summarize(mean_P1 = mean(P1))
```

**Exercise 3:**
Add to the above result the standard deviation and CV for variable `P1`, by adding additional arguments to `summarize`

```{r, eval=FALSE}
precision %>%
  

```

```{r, echo=FALSE}
precision %>%
  filter(Analyte == "AFP") %>%
  summarize(mean_P1 = mean(P1),
            sd_P1 = sd(P1),
            CV_P1 = sd(P1) / mean(P1) * 100)
```

**End exercise**

### Visualize 

**Exercise 4:**
Plot histogram of these results with vertical lines marking parametric 95% central range. Consider setting the `binwidth`

```{r, eval=FALSE}
g <- precision %>%
  filter(Analyte == ________) %>%
  ggplot()
g + geom_histogram(aes(x=________), binwidth=_________) +
  geom_vline(aes(xintercept = mean(P1) + 2 * sd(P1)), linetype=2, color="blue") +
  geom_vline(aes(xintercept = _________), linetype=2, color="blue")
```

```{r, echo=FALSE}
g <- precision %>%
  filter(Analyte == "AFP") %>%
  ggplot()
g + geom_histogram(aes(x=P1), binwidth=0.5) +
  geom_vline(aes(xintercept = mean(P1) + 2 * sd(P1)), linetype=2, color="blue") +
  geom_vline(aes(xintercept = mean(P1) - 2 * sd(P1)), linetype=2, color="blue")
```

**End exercise**

As expected, interday replicates of QC samples look relatively normally distributed. The other way we classically look at such QC results is longitudinally. Let's plot results over time using `geom_point`

```{r}
g <- precision %>%
  filter(Analyte == "AFP") %>%
  ggplot()
g + geom_point(aes(x=Sample, y=P1))
```

**Exercise 5:** 
Let's customize this plot to make it look a bit more useful

- Add horizontal lines using `geom_hline` for the mean, +/- 1 SD, and +/-1 2SDs
- Change the y-axis range using `ylim` to 6 - 10

```{r, eval=FALSE}
g <- precision %>%
  filter(Analyte == "AFP") %>%
  ggplot()
g + geom_point(aes(x=Sample, y=P1)) +
  geom_hline(aes(yintercept = mean(P1)), linetype=1, color="blue") +
  geom_hline(aes(yintercept = mean(P1) + 1 * sd(P1)), linetype=3, color="blue") +
  geom_hline(aes(yintercept = mean(P1) + 2 * sd(P1)), linetype=2, color="blue") +
  geom_hline(_______) +
  geom_hline(_______________) +
  ylim(_________)
```

```{r, echo=FALSE}
g <- precision %>%
  filter(Analyte == "AFP") %>%
  ggplot()
g + geom_point(aes(x=Sample, y=P1)) +
  geom_hline(aes(yintercept = mean(P1)), linetype=1, color="blue") +
  geom_hline(aes(yintercept = mean(P1) + 1 * sd(P1)), linetype=3, color="blue") +
  geom_hline(aes(yintercept = mean(P1) + 2 * sd(P1)), linetype=2, color="blue") +
  geom_hline(aes(yintercept = mean(P1) - 1 * sd(P1)), linetype=3, color="blue") +
  geom_hline(aes(yintercept = mean(P1) - 2 * sd(P1)), linetype=2, color="blue") +
  ylim(6, 10)
```

Is this imprecision acceptable? ...depends on analytical and clinical goals.

### LOB  (Optional)

Limit of the Blank is the minimum concentration that a sample without the analyte will rarely be as high as. We often approximate this as the 95th percentile of distribution of results from measuring a BLANK sample.

Unfortunately, our loaded datasset has no measures of a blank sample: 

```{r}
precision$BLANK
```

So, let's simulate some. Let's simulate normally distributed data with mean 1.6 and standard deviation of 1.2 using `rnorm`.

```{r}
n_samples <- 1e4
set.seed(13)   # Make `random` data simulation reproducible

x <- rnorm(n=n_samples, mean = 1.6, sd = 1.2)    # Generate simulated data
head(x)

sim_precision <- tibble(sample = 1:n_samples,  # Put data into a table
                   x = x)
head(sim_precision)
```

Spot check the histogram of this simulated data:

```{r}
ggplot(data=sim_precision) +
  geom_histogram(aes(x=x)) +
  geom_vline(aes(xintercept = mean(x) + 2 * sd(x)), linetype=2, color="blue") +
  geom_vline(aes(xintercept = mean(x) - 2 * sd(x)), linetype=2, color="blue")
```

Note that there are simulated results less than 0. As an aside, here is one option for handling these:

```{r}
tmp <- sim_precision
mask <- tmp$x < 0              # boolean mask
observation_vec <- which(mask)   # vector of affected (TRUE) columns
tmp$x[observation_vec] <- 0      # Convert these to 0

# replot
ggplot(data=tmp) +
  geom_histogram(aes(x=x)) +
  geom_vline(aes(xintercept = mean(x) + 2 * sd(x)), linetype=2, color="blue") +
  geom_vline(aes(xintercept = mean(x) - 2 * sd(x)), linetype=2, color="blue")
```

**Exercise 6:** 
Calculate LOB as 1.645 SDs above mean of the blank
```{r, eval=FALSE}
mean(________)
sd(___________)
_______________________  # LOB
```

```{r, echo=FALSE}
mean(sim_precision$x)
sd(sim_precision$x)
mean(sim_precision$x) + 1.645*sd(sim_precision$x)
```

Alternatives for similar calculations
```{r}
quantile(x, probs = 0.95)  # non-parametric 95th percentile
mean(x) + qnorm(0.95) * sd(x)   # Extract the SD factor for the 95th percentile in normal distribution
```

**End exercise**

## Calibration Verification

### Load data for AFP

```{r}
afp <- read_xlsx(path = "data/Method_Validation.data.xlsx", 
                  sheet = "Linearity") %>%
  filter(Test == "AFP")
str(afp)
head(afp)
```

Calculate average result for each set of sample test replicates using `mutate`
```{r}
afp <- afp %>% 
  mutate(Observed = (Result_1 + Result_2 + Result_3) / 3)
```

### Visualize calibration verification

Plot calverification results for AFP
```{r}
ggplot(data=afp) +
  geom_point(aes(x=Assigned_Value, y=Observed)) +
  geom_abline(slope=1, intercept=0, linetype=2, color="gray")
```

How far off are observed values from assigned?
```{r}
afp <- afp %>%
  mutate(value_diff = Observed - Assigned_Value) %>%
  mutate(value_percent_diff = value_diff / Assigned_Value * 100,
         recovery = Observed / Assigned_Value * 100)

afp$value_percent_diff
afp$recovery
```

Do these meet our goals? 

- Let's apply a simple goal of % difference < 30%. 
- Do all dilutions meet this threshold?
- Note use of `abs` for absolute value
```{r}
tmp <- afp %>%
  mutate(pass_calvar = abs(value_percent_diff) < 30) 

# Visualize the relevant variables
tmp %>%
  select(Sample, value_percent_diff, recovery, pass_calvar)

# Display the failed observations
tmp %>%
  filter(!pass_calvar)
```

**Exercise 7:** 
Seems that our acceptability criteria was too simplistic, because it only considered relative differences. 

- What about criteria for %difference < 30% or absolute difference < 1? 

> Note: multiple boolean vectors can be combined together using the bitwise OR operator `|` (e.g. `(is.na(A)) | (A < 5)).

```{r, eval=FALSE}
afp %>%
  mutate(pass_calvar = (abs(value_percent_diff) < 30) _____________) %>%
  filter(!pass_calvar)
```

```{r, echo=FALSE}
afp %>%
  filter(Test == "AFP") %>%
  mutate(pass_calvar = (abs(value_percent_diff) < 30) | (abs(value_diff) < 1)) %>%
  filter(!pass_calvar)
```

**End exercise**

Let's plot these absolute differences
```{r}
tmp <- afp

# Figure out plot symmetric y-axis limits
max_diff <- abs( max(tmp$value_diff, na.rm=T) ) 

ggplot(data=tmp) +
  geom_point(aes(x=Assigned_Value, y=value_diff)) +
  geom_hline(yintercept = 0, linetype=1, color="gray") +
  ylim(-max_diff, max_diff) +
  ylab("Observed - Assigned")       # Change y-axis label
```

**Exercise 8:** 
Adapt the above code to plot the percent differences

```{r, eval=FALSE}
g <- ggplot(___________) +
  geom_point(_______) +
  geom_hline(yintercept = 0, linetype=1, color="gray") +
  ylim(____________) +
  ylab(_________________)
g
```

```{r, echo=FALSE}
g <- ggplot(data=tmp) +
  geom_point(aes(x=Assigned_Value, y=value_percent_diff)) +
  geom_hline(yintercept = 0, linetype=1, color="gray") +
  ylim(-100, 100) +
  ylab("% Observed - Assigned")
g
```

**End exercise**

## Linearity (optional)

The dataset gives us the assigned values for each sample, so we can back calculate the expected ratios between each sample based on dilution
```{r}
afp <- afp %>%
  mutate(dilution = Assigned_Value / max(Assigned_Value))
afp$dilution
```

**Exercise 9:** 
Calculate each samples results based on expected dilution factor and observed result in S6. 

- Assume there is no contribution from S0 in the mixing experiment

```{r, eval=FALSE}
Observed_S6 <- max(____________)
afp <- afp %>%
  mutate(expected_result = _______________)

afp %>%
  select(Sample, Observed, expected_result)
```

```{r, echo=FALSE}
Observed_S6 <- max(afp$Observed)
afp <- afp %>%
  mutate(expected_result = Observed_S6 * dilution)

afp %>%
  select(Sample, Observed, expected_result)
```

**End exercise**

Plot linearity results
```{r}
ggplot(data=afp) +
  geom_point(aes(x=expected_result, y=Observed)) +
  geom_abline(slope=1, intercept=0, linetype=2, color="gray")
```

Evaluate linearity results
```{r}
tmp <- afp %>%
  mutate(value_diff = Observed - expected_result) %>%
  mutate(percent_value_diff = value_diff / expected_result * 100,
         recovery = Observed / expected_result * 100) %>%
  mutate(pass_linearity = (abs(value_diff) < 1) | (abs(percent_value_diff) < 30))

# Visualize the relevant variables
tmp %>%
  select(Sample, value_diff, percent_value_diff, recovery, pass_linearity)

# Visualize the relevant observations
tmp %>%
  filter(!pass_linearity)
```
