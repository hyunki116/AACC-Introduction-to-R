---
title: "Method Validation in R -- Method Comparison"
date: "06/07/2018"
output:
  pdf_document: default
  html_document: default
authors:
- Niklas Krumm
- Daniel Herman
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NA)
library(tidyverse)
library(readxl)
```

# Method Validation in R: Method Comparison

## Overview

In this section we will continue exploring how to use R in method validation by comparing results from two different methods.

## Load data

Let's load in hCG data, just as we did in the previous session.
```{r}
hcg <- read_excel(path="data/Method_Validation.data.xlsx", 
                sheet="MS HCG")
glimpse(hcg)
```

## Describe data and explore its distribution

Let's use pipes to summarize and calculate a few statistics:

```{r}
hcg %>%
  summarize(method_a_mean = mean(method_a), 
            method_a_sd = sd(method_a),
            method_b_mean = mean(method_b), 
            method_b_sd = sd(method_b))
```

## Overlapping histograms

What if we want to plot the distribution of both `method_a` and `method_b` in the same plot? We've used `geom_histogram` previously. Let's try the related ggplot function `geom_freqpoly`, starting with a single method.

```{r}
ggplot(data = hcg) + 
  geom_freqpoly(bins=20, 
                aes(x=method_a))
```

Now, let's add a second method and mark the two using `geom_freqpoly`.

```{r}
ggplot(data = hcg) + 
  geom_freqpoly(bins=20, aes(x=method_a, color="blue")) +
  geom_freqpoly(bins=20, aes(x=method_b, color="red")) 
```

**Exercise 1:**

Make a similar display of method a and method b distributions using the `geom_density` function. Set the `fill` and `color` functions to distinguish between the two methods. Test using the `alpha` parameter to increase the shape translucency

```{r, eval=FALSE}
ggplot(data = hcg) + 
  
  
```

```{r, echo=FALSE}
ggplot(data = hcg) + 
  geom_density(aes(x=method_a, fill="blue", color="blue"), alpha=0.5) +
  geom_density(aes(x=method_b, fill="red", color="red"), alpha=0.5)

```

**End exercise**

### (Optional) Tibble reorganization: `gather`
Now, these plots required to separate calls to the `geom_*` functions for each method. This works, but makes legends needing manual cleanup and does not scale well.

Another way is to create a "long dataframe" using the `gather` function. This "unpacks" multiple columns into just two columns, where the **first column** is the `key` and the **second column** is the `value`:
[assets/data_gather.png]()
```{r}
long_hcg <- hcg %>%
  gather(key="method", value="value", 
         -specimen)
head(long_hcg)
```

Take a moment to compare the `hcg` and `long_hcg` objects. How are they different? Note that when we have a "long" datafarme, every row is a named observation (also known as a key-value pair). For reference, the reverse transformation (from "long" to "wide") is done with the `spread` function.

Once in a long-form format, we can tell ggplot to use both the `value` variable for our x-axis and the `method` variable to set the coloring (fill and color) and the resulting legend.

```{r}
ggplot(long_hcg) + 
  geom_density(aes(x=value, fill=method, color=method), 
               alpha=0.5)
```

## Method comparison (t-tests, and more)

### Using a statistical test
R is a statistical programming language, so simple statistical testing is straightforward:
```{r}
# Note we are using the paired=TRUE variant of the t.test, since we have paired measurements.
t.test(hcg$method_a, hcg$method_b, 
       paired=TRUE)
```

For more information on the `t.test` function, (follow this link)[https://www.statmethods.net/stats/ttest.html].

**Exercise 2:**
Evaluate parametric comparability of method means after log-transformation

```{r, eval=FALSE}
# Note we are using the paired=TRUE variant of the t.test, since we have paired measurements.
t.test(___________, ____________, 
       paired=TRUE)
```

```{r, echo=FALSE}
# Note we are using the paired=TRUE variant of the t.test, since we have paired measurements.
t.test(log(hcg$method_a), log(hcg$method_b), 
       paired=TRUE)
```
**End exercise**

### Using the RIGHT statistical test
Is `t.test` the right function? Consider the histograms above and our previous work with log normalizing the values. 

|Populations|Parametric|Non-parametric|
|:-------------|:-------------------------:|:-------------------------:|
|Two populations|t-test|Mann-Whitney U|
|Many populations|ANOVA|Kruskal Wallis / one-way anova|
|Populations across several treatments/times|repeated measures ANOVA|Friedman test|

**Exercise 3:** 
Using the table above, select the _right_ test for comparing `method_a` and `method_b`. Look up the function call using google, R documentation or any other source. Write out the function and calculate a p-value below

```{r, eval=FALSE}
hcg %>%
  
```

```{r, echo=FALSE}
wilcox.test(hcg$method_a, hcg$method_b, paired=TRUE)
```
**End Exercise** 

## Regression

### Simple linear regression

Let's begin by simply plotting `method_a` and `method_b` as a scatter plot. Notice how we are using the `aes()` to define "mappings" from our data to the x and y coordinates:
```{r}
ggplot(hcg) + 
  geom_point(aes(x=method_a, y=method_b))
```


Adding a least-squares regression line is easy with a little bit of magic from `ggplot`. The `lm` (Linear Model) function does all the work here!

```{r}
ggplot(hcg) + 
  geom_point(aes(x=method_a, y=method_b)) + 
  geom_smooth(method = "lm", aes(x=method_a, y=method_b))
```

It's tough to see the fit in the low result range, so we can transform our axis:

```{r}
ggplot(hcg) + 
  geom_point(aes(x=method_a, y=method_b)) + 
  geom_smooth(method = "lm", aes(x=method_a, y=method_b)) +
  scale_x_log10() + scale_y_log10()
```

What if we want to just extract the coefficients of the linear model? We can utilize R's formula notation format and the `lm` function:

```{r}
regression <- lm(method_b ~ method_a, hcg)
summary(regression)
coef(regression)
```

## Deming regression

In fact, a least-squares regression, while a good approximation for this type of data, assumes that the x-dimension has no measurement error so it minimizes errors only in the y-dimension. The *Deming regression* differs from the simple linear regression in that it accounts for errors in observations on both the x- and the y- axes, thus making it more suitable for estimating a best-fit line between two measured variables.

```{r}
library(mcr)            # remember, you may need to install.packages("mcr") in the console first!
deming_results <- mcreg(hcg$method_a, hcg$method_b, 
                        method.reg = "Deming")
deming_results@para         # "para" short for "parameters"-- this is a library/method specific term here
```

To see what slots (attributes) `deming_results` has beyond `para`, look at `glimpse(deming_results)`

**Exercise 4:**

Another issue in standard regression is if the random error scales with analyte concentration, then the observed absolute errors are higher for the higher concentrations. This means that error at the high-end is more heavily factored into the regression fit. One way of adjusting for this is weighting errors for high concentrations less. 

Run the regression using the weighted deming method of the `mcreg` function. How do the slope and intercept differ?

```{r, eval=FALSE}
wdeming_results <- 
wdeming_results@para         # "para" short for "parameters"-- this is a library/method specific term here
```

```{r, echo=FALSE}
wdeming_results <- mcreg(hcg$method_a, hcg$method_b, 
                        method.reg = "WDeming")
wdeming_results@para         # "para" short for "parameters"-- this is a library/method specific term here
```

**End exercise**

Now let's add the lines to our plot. We can use the `geom_abline()` ggplot function to add a line with a slope and intercept. The intercept and slope are stored in `deming_results@para[1]` and `deming_results@para[2]`, respectively.

```{r}
ggplot(hcg) +
  geom_point(aes(x=method_a, y=method_b))  +
  geom_smooth(method = "lm", aes(x=method_a, y=method_b), se=FALSE) +
  geom_abline(intercept = deming_results@para[1], slope = deming_results@para[2], color="red")
```

What about the weighted deming fit line? What about the subrange between 0 and 40000? Let's also add a 1:1 line to help interpret fit.

```{r}
ggplot(hcg) +
  geom_point(aes(x=method_a, y=method_b))  +
  geom_smooth(method = "lm", aes(x=method_a, y=method_b), se=FALSE) +   #blue
  geom_abline(intercept = deming_results@para[1], slope = deming_results@para[2], color="red") +
  geom_abline(intercept = wdeming_results@para[1], slope = wdeming_results@para[2], color="yellow") +
  xlim(0, 40000) + ylim(0, 40000) +
  geom_abline(intercept=0, slope=1, linetype=2, color="gray")
  
```


## Passing-Bablock

```{r}
PB_results <- mcreg(hcg$method_a, hcg$method_b, method.reg = "PaBa")
PB_results@para
```

**Exercise 5:**
Add another `geom_abline` to the plot above for the Passing-Bablock regression coefficients determined above.

```{r, eval=FALSE}
ggplot(hcg) +
  geom_point(aes(x=method_a, y=method_b))  +
  geom_smooth(method = "lm", aes(x=method_a, y=method_b), se=FALSE) +  #blue
  geom_abline(intercept = deming_results@para[1], slope = deming_results@para[2], color="red") +
  xlim(0, 40000) + ylim(0, 40000) +
  geom_abline(intercept=0, slope=1, linetype=2, color="gray") +
  geom_abline(____________________)
```

```{r, echo=FALSE}
ggplot(hcg) +
  geom_point(aes(x=method_a, y=method_b))  +
  geom_smooth(method = "lm", aes(x=method_a, y=method_b), se=FALSE) +  #blue
  geom_abline(intercept = deming_results@para[1], slope = deming_results@para[2], color="red") +
  xlim(0, 40000) + ylim(0, 40000) +
  geom_abline(intercept=0, slope=1, linetype=2, color="gray") +
  geom_abline(intercept = PB_results@para[1], slope = PB_results@para[2], color="yellow")
```

**End Exercise**

### Extra-credit: Outlier robustness
How "robust" are each of these methods to outliers? Let's try it out.
```{r}
# Step 1: make a copy of the data so we don't change the original
hcg_with_outliers <- hcg

# Step 2: modify the data to include some outliers for method_a (fake data!)
hcg_with_outliers$method_a[10:12] <- 100000

# Step 3: Re-run fits
deming_results_outliers <- mcreg(hcg$method_a, hcg$method_b, 
                        method.reg = "Deming")
PB_results_outliers <- mcreg(hcg$method_a, hcg$method_b, method.reg = "PaBa")

# Step 3: same plotting code as above, using our new fake data
ggplot(hcg_with_outliers) +
  geom_point(aes(x=method_a, y=method_b))  +
  geom_smooth(method = "lm", aes(x=method_a, y=method_b), se=FALSE) +   #blue
  geom_abline(intercept = deming_results_outliers@para[1], slope = deming_results_outliers@para[2], color="red") +
  geom_abline(intercept = PB_results_outliers@para[1], slope = PB_results_outliers@para[2], color="green")

```

That is quite a difference!

## Compare methods by concordance relative to decision thresholds
Next, let's compare method A and B using decision thresholds. For the purpose of this tutorial, we will simply use 25,000 as our threshold.

```{r}
threshold <- 25000

tmp <- hcg %>%
  mutate(method_a_pos = method_a > threshold,   # Create binary indicator for method_a
         method_b_pos = method_b > threshold)
  table(x=tmp$method_a_pos, y=tmp$method_b_pos)
```
Looking at this table, method_a and method_b are *discordant* across our threshold in 40 cases, and *concordant* in 58 + 48 cases.

A tidy way to do this without using `tmp` and `table` is:

```{r}
threshold <- 25000

hcg %>%
  mutate(method_a_pos = method_a > threshold,   # Create binary indicator for method_a
         method_b_pos = method_b > threshold) %>%
  count(method_a_pos, method_b_pos)
```

Now to convert this into a standard concordance table:

```{r}
hcg %>%
  mutate(method_a_pos = method_a > threshold,   # Create binary indicator for method_a
         method_b_pos = method_b > threshold) %>%
  count(method_a_pos, method_b_pos)  %>%
  spread(method_b_pos, n, fill=0, sep=".")   # Spreads method_b_pos from a single variable to a variable for each value
```


**Exercise 6**: Write code to compare accuracy across two different decision thresholds (25000 and 50000, for example)

_Hint #1_: In the `mutate` function, use the `cut()` function to break a numerical range into multiple a set of factor levels (categories):
     For instance, for method_a you could run `cut(hcg$method_a, breaks=c(0, 20, 40, Inf), labels=c("low","middle","high"))` to convert to a factor for 0-20, 20-40, 40-Inf.

_Hint #2_: Look at previous code for inspiration!

```{r, eval=FALSE}
hcg %>%
  mutate(method_a_bin = ________________________________,   # Create factor indicator for method_a
         method_b_bin = _________________________________) %>%
  count(method_a_bin, method_b_bin)  %>%
  spread(method_b_bin, n, fill=0, sep=".")   # Spreads method_b_pos from a single variable to a variable for each value
```

```{r, echo=FALSE}
hcg %>%
  mutate(method_a_bin = cut(method_a, 
                            breaks=c(-Inf, 25000, 50000, Inf), 
                            labels=c("low","middle","high")),   # Create factor indicator for method_a
         method_b_bin = cut(method_b, 
                            breaks=c(-Inf, 25000, 50000, Inf), 
                            labels=c("low","middle","high"))) %>%
  count(method_a_bin, method_b_bin)  %>%
  spread(method_b_bin, n, fill=0, sep=".")   # Spreads method_b_pos from a single variable to a variable for each value
```

**End Exercise** 
