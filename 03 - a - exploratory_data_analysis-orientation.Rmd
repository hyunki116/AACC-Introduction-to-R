---
title: "Exploratory data analysis: Orienting yourself with your data set"
author: "Patrick Mathias"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
library(tidyverse)
library(readxl)
library(janitor)
library(lubridate)
```

# Exploratory data analysis: Orienting yourself with your data set

In this section of the course we will walk through a common workflow: exploring a new data set. Before we dive into the data set, we will touch briefly on one way of writing code that can help make it easier to follow.

## Sequencing functions

When you are working with a data set, you often need to manipulate it multiple times in a defined sequence of events. Let's start with a non-sensical example that can help illustrate the issue (adapted from the [tidyverse style guide](http://style.tidyverse.org/pipes.html)).

Let's say we want to apply the functions hop, scoop, and bop to the foo_foo data frame, in that order. One way to approach that is to start with the data, apply the function, and write the output back into the original data frame.

```{r, eval = FALSE}
# one way to represent a hop, scoop, and a bop, without pipes
foo_foo <- hop(foo_foo, through = forest)
foo_foo <- scoop(foo_foo, up = field_mice)
foo_foo <- bop(foo_foo, on = head)
```

R allows you to nest functions within one another, but this can get horribly confusing because following a specific sequence of operations requires you to start from the inside of the expression and expand outwards.

```{r, eval = FALSE}
# another way to represent the same sequence with less code but in a less readable way
foo_foo <- bop(scoop(hop(foo_foo, through = forest), up = field_mice), on = head)
```

You want to try and avoid doing things this way because the sequence of operations is so non-intuitive.

Explicitly showing the functions sequentially by line is helpful for readability but it does require some unnecessary typing to keep repeating the name of the data set. R allows you to "pipe" a data frame from one function to another using this funny looking operator: `%>%`. This can cut down on unnecessary code but also preserves the nice formatting that makes it obvious what functions are applied in what order.

```{r, eval = FALSE}
# a hop, scoop, and a bop with the almight pipes
foo_foo %>%
  hop(through = forest) %>%
  scoop(up = field_mouse) %>%
  bop(on = head)
```

Pipes are not compatible with all functions but should work with all of the tidyverse package functions (the magrittr package that defines the pipe is included in the tidyverse). In general, functions expect data as the primary argument and you can think of the pipe as feeding the data to the function. From the perspective of coding style, the most useful suggestion for using pipes is arguably to write the code so that each function is on its own line. The tidyverse style guide [section on pipes](http://style.tidyverse.org/pipes.html) is pretty helpful.

## Loading data and reviewing data types

First let's refresh your memory on loading in a data set. We have an Excel file that contains our main data set in the data folder called "orders_data_set.xlsx". After loading the file into a variable (in this case a data frame) called "orders", look at the structure of the data using the `str()` function.

```{r load_data}
orders <- read_excel("data/orders_data_set.xlsx")
str(orders)
```

**Exercise**

1. Which fields of the data frame are characters?
 
2. Which are numbers?

3. Of those fields above, which would be best represented as factors?

4. Let's start by running a summary (`summary`) of one of those character fields that we think should be a factor. What information can we determine from that summary?

5. Let's convert one of those fields to a factor using the `as.factor()` command and then run a summary. What additional information do you see by converting to a factor?

**End Exercise**

Tip: White space generally has meaning in programming. When a variable name has a space in it, you can use the `` ` `` character (look to the top left on your keyboard) around the variable name to make sure R understands what variable you are referring to. As an example, `summary(orders$Proc Code)` will not work but `summary(orders$`Proc Code`)` will.

White spaces in names can be annoying to deal with. To get around this, we can rename variable names to remove white spaces, and, in addition, we can convert everyting to a single case (lowercase by default). The janitor package has some handy data science tools, including the ability to clean up variable names in one line using the `clean_names` function:

```{r load_data_clean_headers}
orders <- read_excel("data/orders_data_set.xlsx") %>%
  clean_names()
str(orders)
```

As you may have seen already, there is more than one way to do the same thing when you're programming. We used `str()` to look at the structure of a data frame or other object. Another way to do this is to use the `glimpse()` function, which produces very similar output but is organized a little more neatly (with 1 line per variable).

```{r glimpse}
glimpse(orders)
```

The `str` and `glimpse` functions are helpful to get a quick snapshot of the data, but sometimes you want a little more detail about the data in your data frame.

```{r summary}
summary(orders)
```

The `summary` function is most useful when you want to quickly glance at distributions of numerical data and times. You can quickly see the minimum and maximum times and some data on the distribution. It is less helpful when you have characters. One way to deal with this issue to convert a character variable into a factor using the `as.factor()`function.

**Exericse**

Pull a summary of the description variable (only), after converting to a factor using the `as.factor` function. Which test is most frequently ordered in this data set?

```{r convert_factor}
summary(as.factor(orders$description))
```

**End Exercise**

## Reshaping rectangular data

Much of the laboratory data we work with has a tibble (data frame) structure, with rows as observations and columns as variables. Sometimes we start with a giant spreadsheet with tens of columns where we only care about a few columns. Sometimes we start with a spreadsheet with thousands or millions of rows and only care about a small subset of those. There are a variety of functions in the dplyr package that help us reshape our rectangular data quickly.

### Make your data skinny with `select()`

When starting with a wide tibble (lots of columns), we can make it skinny by selecting specific columns using the `select()` function. In addition to supplying the data frame as the first argument (not needed if using the pipe), `select()` expects the names of the variables you would like to extract.

```{r select}
orders_skinny <- orders %>%
  select(order_id, patient_id, description, order_time)
glimpse(orders_skinny)
```

### Shorten your tall data with `filter()`

Alternately, we might have a long tibble (lots of rows) but only want to select specific rows meeting some criteria. One of the most useful functions in the dplyr package is `filter()`, which allows you to select specific rows from a data frame. The arguments to the function include the data frame (which can be skipped if you use a pipe) and then one or more conditions to select the rows you want.

Let's extract the rows associated with complete blood count orders. The procedure code for those rows is "CBC", so the condition we apply to the filter function is `proc_code == CBC`. Note that we use two equal signs instead of one to indicate an equality condition - you will get an error if you use a single equal sign.

```{r filter}
cbc_orders <- orders %>%
  filter(proc_code == "CBC") # note the two equal signs for evaluating equality (because one equal is for assignment)
```

With that step, we have now create a tibble that only has the rows with a CBC order.

### Sort your data with `arrange()`

In some cases we do not want to remove rows from our tibble but would like to re-order the rows to manually review the data or perform some other operations. The `arrange()` function can order the rows by the variables you specify. Be default, it will sort from smallest to largest, but you can also sort in descending order using the `desc()` function as an argument.

We may want to sort first by patient ID, then by order time.

```{r arrange}
orders_arranged <- orders %>%
  arrange(patient_id, order_time)
```

### Add extra columns with `mutate()`

Earlier we saw that factors can be helpful when summarizing data. What if we wanted to permanently convert one of our variables to a factor within the data frame? We could do this many different ways, but let's start by creating a new variable (column) using the `mutate()` function. `mutate()` allows you to perform a function on one or more variables to create (or overwrite) a variable within the same observation (row).

Here we take our orders data frame, pipe it into the `mutate()` function and create a new variable called department_fctr that applies the `as.factor` function on our original department variable. We can run the `summary` function to see the results.

```{r single_factor}
orders <- orders %>%
  mutate(department_fctr = as.factor(department))
summary(orders$department_fctr)
```

**Exercise**

Let's go ahead and convert the description variable to a factor using the `mutate` and `as.factor` functions. This time, rather than creating a new variable, overwrite the original variable by giving it its original name.

```{r single_factor_exercise}
orders <- orders %>%
  mutate(description = as.factor(description))
summary(orders$description)
```

**End Exercise**

Factors can be very handy when you want to look at quick summaries of the data. In some cases you may want all of your variables that are characters to actually be factors. We're jumping into a little more advanced concepts, but let's briefly cover one way to convert multiple variables into factors at once.

If we decide to make description, proc_code, order_class_c_descr, lab_status_c_descr, order_status_c_descr, and reason_for_canc_c_descr all into factors, we could use `mutate` and call out every variable (on separate lines for readability):

```{r convert_all_factors_long}
orders_factors <- orders %>%
  mutate(description = as.factor(description),
         proc_code = as.factor(proc_code),
         order_class_c_descr = as.factor(order_class_c_descr),
         lab_status_c_descr = as.factor(lab_status_c_descr),
         order_status_c_descr = as.factor(order_status_c_descr),
         reason_for_canc_c_descr = as.factor(reason_for_canc_c_descr))
summary(orders_factors)
```

An extension of the `mutate` function is `mutate_at`, which serves the same purpose but allows you to choose multiple columns at once. If you're applying the same function to multiple columns, this is a handy way to do that with less code.

```{r convert_all_factors}
orders <- orders %>%
  mutate_at(c("description", "proc_code", "order_class_c_descr", "lab_status_c_descr", "order_status_c_descr", "reason_for_canc_c_descr"), as.factor)
summary(orders)
```

Yet another way to do this is to use `mutate_if(is.character, as.factor)` but beware! 

Older functions to import files in R automatically convert character variables into factors. This can be helpful for variables like order status where there are only a handful of different possible values for the variable (called "levels" of a factor). But factors behave differently than characters, and your code can produce unexpected output instead of failing.

## Tabulating our data

A very common task when analyzing data is tabluating counts based on one or more variables. In Excel this is commonly handled with pivot tables. R has multiple functions that can help you quickly create tables. To demonstrate some tabulations, let's first focus on a single test and return to our CBC orders tibble we created in the previous section. We want to tabulate the counts of CBC orders based on a single variable, department, ie. we wnat ot determine the number of CBCs ordered by each department. To start out, we use the `table` function in base R (no additional package is required to call the function).

```{r cbc_simple_tabulation}
table(cbc_orders$department) # recall the dollar sign syntax to indicate a variable from an object
```

Now let's generate a more complex table to visualize the number of CBC orders by department AND split out by order status.

```{r cbc_complex_tabulation}
table(cbc_orders$department, cbc_orders$order_status_c_descr)
```

**Exercise**

Let's practice making tables. This time, create a table showing the breakdown of CBC with differential (procedure code CBD) by department and order class. Which clinics draw a majority of their own samples? Which clinics use the most external orders (ie. use results from outside labs)?

```{r cbc_analysis}
cbd_orders <- orders %>%
  filter(proc_code == "CBD")
table(cbd_orders$department, cbd_orders$order_class_c_descr)
```

**End Exercise**


### Taking a quick look at your data with `skim()` (Optional)

The skimr package is worth knowing about. This does some work for you in breaking down distributions of different variables and showing the amount of missing data.

```{r skim}
install.packages("skimr")
library(skimr)
skim(orders)
```

### Creating more complicated tables (Optional)

We used the janitor package earlier to help clean up variable (column) names. This package also includes helpful functions for tabulating. The basic `tabyl` funcationality works similarly to the `table` function in base R, but the syntax of the arguments is different. Rather than explicitly calling the combination of object and variable name (indicated by the "$"), the `tabyl` function adopts the syntax we've seen before where the first argument is the object (tibble in this case) that can be piped in, and the variables can serve as inputs to the function without calling the name of the object.

```{r basic_tabyl}
cbc_orders %>% 
  tabyl(department, order_status_c_descr)
```

The basic functionality of `tabyl` does not improve much over the base `table` function, but there are a series of helper functions beginning with `adorn_` that can modify the table output to be something more helpful. In the example below, we use `adorn_percentages` to convert from raw counts to percentages calculated across the rows. We also include the raw counts using the `adorn_ns()` function to see the full data set.

```{r tabyl}
cbc_orders %>% 
  tabyl(department, order_status_c_descr) %>%
  adorn_totals("row") %>% # tabulate operations below across rows
  adorn_percentages("row") %>% # express counts as percentages
  adorn_pct_formatting() %>% # clean up percentages for nicer printing
  adorn_ns() # add back in counts (N's)
```

The janitor package is very useful for cleaning up dirty, manually curated spreadsheets. You can read more about it [here](https://github.com/sfirke/janitor).

## Simple summary visualizations

One of R's greatest strengths is its ability to create complex visualizations without writing a large amount of complex code. When we reviewed our method validation data, we were oriented to the ggplot2 package. With our orders data, we can dive into how plots using this package work.

The general syntax for plotting with ggplot follows this structure:

```{r, eval = FALSE}
ggplot(data = <tibble>) +
  <geom_function>(mapping = aes(<mapping(s)>))
```

The data is expected to be in a tibble (data frame) where each row represents a unique observation and each column represents a distinct variable ("tidy" data - for more on tidy data info read [here](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html)). Code for plotting with this package includes the following components:

1. The `ggplot()` function identifies the data set you want to plot 
2. A variety of different geom functions produce the visualizations that you layer on top of your data set
3. The input for these geom functions is a mapping of which variables to plot

Let's look at a simple visualization of our orders data set, so our orders data set is the input for the `ggplot()` function. We are interested in the breakdown of order volumes by department. One straight-forward visualization is a bar chart, so we use the `geom_bar()` function and provide the department as the input into the `aes()`, which is an aesthetic. In this case, we provide department as the x-axis. Note that the geom function follows the `ggplot()` function after a "+" (this is a distinct situation from using pipes).

```{r}
ggplot(data = orders) + 
  geom_bar(mapping = aes(x = department))
```

The definitely shows the data, but the x-axis is not readable because there are so many categories. The easiest solution is to flip the axes using the `coord_flip()` function.

```{r}
ggplot(data = orders) + 
  geom_bar(mapping = aes(x = department)) +
  coord_flip()
```

**Exercise**

1. Plot a bar graph of the orders data set showing the breakdown of order status.

```{r, eval = FALSE}
ggplot(data = orders) + 
  geom_bar(mapping = aes(x = order_status_c_descr)) 
```

2. Plot a bar graph showing the breakdown of order class.

```{r, eval = FALSE}
ggplot(data = orders) + 
  geom_bar(mapping = aes(x = order_class_c_descr)) 
```

**End Exercise**

Plotting order volumes as a function of time can be helpful across a variety of settings: monitoring a utilization intervention, analyzing seasonality for specific tests, creating projections for workload, etc. We can revisit the bar graph but map the time of order to the x-axis.

```{r}
ggplot(data = orders) + 
  geom_bar(mapping = aes(x = order_time))
```

This bar chart looks very uneven, with bars that are spread far apart. Let's try a geom function that is related to `geom_bar()` but intended to visualize distributions of continuous variables like time: `geom_histogram()`.

```{r}
ggplot(data = orders) + 
  geom_histogram(mapping = aes(x = order_time))
```

Note that R output a warning: "`stat_bin()` using `bins = 30`. Pick better value with `binwidth`." By default `geom_histogram()` splits the continous variable into 30 bins, which makes the bars look very uneven over time. That split is arbitrary and probably does not reflect the true distribution of data.

How do we fix this? The `geom_histogram()` function uses seconds as its default unit of time (but will use days if the variable is a date rather than a time), so we can define a binwidth that is multiples of seconds to reflect days or weeks. For example, one day = 60*60*24 seconds.

```{r}
ggplot(data = orders) + 
  geom_histogram(mapping = aes(x = order_time), binwidth = 60*60*24)
```

That pattern looks more accurate, with decreased volumes occuring on weekends. Alternately, we can look at volumes by week instead of by day.

```{r}
ggplot(data = orders) + 
  geom_histogram(mapping = aes(x = order_time), binwidth = 60*60*24*7)
```

What if we want to change the x-axis label of our plot? We simply add another layer called `xlab()` and add the string we want to insert.

```{r}
ggplot(data = orders) + 
  geom_histogram(mapping = aes(x = order_time), binwidth = 60*60*24*7) +
  xlab("Time")
```

Now let's add another mapping to our plot. We may want to analyze the number of orders that are cancelled and visualize that alongside the total number of orders. We simply add another variable to the aesthetic function, and instead of plotting against an axis, we are going to fill in the bars with colors based on order status.

```{r}
ggplot(data = orders) +
  geom_histogram(
    mapping = aes(x = order_time, fill = order_status_c_descr),
    binwidth = 60*60*24*7
  ) +
  xlab("Time")
```

**Exercise**

Plot the weekly volume of a subset of common orders: basic metabolic panel, hemoglobin A1C, lipid panel, and TSH (proc_codes BMP, A1C, LIPID, TSH) and show the breakdown of tests by fill in the bar chart.
 
```{r}
orders_subset <- orders %>%
  filter(proc_code %in% c("BMP", "A1C", "LIPID", "TSH"))
ggplot(data = orders_subset) +
  geom_histogram(
    mapping = aes(x = order_time, fill = proc_code),
    binwidth = 60*60*24*7
  ) +
  xlab("Time")
```

**End Exercise**